{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"jOCF9tKlIspw"},"source":["### 1.Naive Bayes Classifier"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"PmuEZmB47rmN"},"outputs":[],"source":["# Tập dữ liệu ví dụ\n","\n","train_x = [\n","           'just plain boring',\n","           'entirely predictable and lacks energy',\n","           'no surprises and very few laughs',\n","           'very powerful',\n","           'the most fun film of the summer'\n","]\n","train_y = [0, 0, 0, 1, 1]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HZp30Q5ZJBSr"},"source":["### 1.1. Tiền xử lý dữ liệu cơ bản"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1665744705919,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"G4_6pcOfI_TN","outputId":"c9645ad0-e757-49b0-9ed1-278014f0606b"},"outputs":[{"data":{"text/plain":["['just', 'plain', 'boring']"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["def basic_preprocess(text):\n","    text_clean = text.lower()\n","    return text_clean.split()\n","\n","basic_preprocess(train_x[0])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JoUOaYc3KFg4"},"source":["### 1.2.Xây dựng bộ từ điển"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"pLBbhdq_LScc"},"outputs":[],"source":["#Ex 1\n","def count_freq_words(corpus, labels):\n","    \n","    model = {}\n","    key = []\n","    for label, sentence in zip(labels, corpus):\n","        for word in basic_preprocess(sentence):\n","            \n","            key = (word, label)\n","            if key in model:\n","                model[key] += 1\n","            else:\n","                model[key] = 1\n","\n","\n","            \n","        \n","    return model"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":392,"status":"ok","timestamp":1665750217589,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"wdDMOM-9MURU","outputId":"b97856c3-2ee9-4737-e9b0-ae2718fb80e4"},"outputs":[{"data":{"text/plain":["{('just', 0): 1,\n"," ('plain', 0): 1,\n"," ('boring', 0): 1,\n"," ('entirely', 0): 1,\n"," ('predictable', 0): 1,\n"," ('and', 0): 2,\n"," ('lacks', 0): 1,\n"," ('energy', 0): 1,\n"," ('no', 0): 1,\n"," ('surprises', 0): 1,\n"," ('very', 0): 1,\n"," ('few', 0): 1,\n"," ('laughs', 0): 1,\n"," ('very', 1): 1,\n"," ('powerful', 1): 1,\n"," ('the', 1): 2,\n"," ('most', 1): 1,\n"," ('fun', 1): 1,\n"," ('film', 1): 1,\n"," ('of', 1): 1,\n"," ('summer', 1): 1}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["\n","freqs = count_freq_words(train_x, train_y)\n","freqs"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":397,"status":"ok","timestamp":1665750238247,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"9Gw126f5pTqJ","outputId":"c0bf84fb-4f42-4785-dcdf-106995469361"},"outputs":[{"data":{"text/plain":["1"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["\n","def lookup(freqs, word, label):\n","\n","    count = 0\n","\n","    pair = (word, label)\n","    if pair in freqs:\n","        count = freqs[pair]\n","\n","    return count\n","\n","lookup(freqs, \"just\", 0)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["3"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["train_y.count(0)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AaSqdNkUNdiI"},"source":["### 1.3.Thuật toán phân loại Naive Bayes\n","\n","- Tính $D$, $D_{pos}$, $D_{neg}$\n","    - Dựa vào `train_y` tính số lượng các sample có trong tập training: $D$, số lượng các sample là positive (nhãn 1): $D_{pos}$ và số lượng nhãn là negative (nhãn 0): $D_{neg}$\n","    - Tính xác suất tiên nghiệm của class 1 là: $P(D_{pos})=D_{pos}/D$, và class 0 là: $P(D_{pos})=D_{pos}/D$"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"tAaPBeBCgDSZ"},"outputs":[],"source":["\n","def compute_prior_prob(train_y):\n","    \n","    D =  len(train_y)\n","    # Tính D_pos, số lượng các positive sample trong training\n","    D_pos = len(list(filter(lambda x : x== 1, train_y)))\n","    # Tính D_neg, số lượng các negative sample trong training\n","    D_neg = len(list(filter(lambda x : x== 0, train_y)))\n","    # Tính xác suất tiên nghiệm cho các class 0 và 1\n","    p_prior = {0:(D_neg/D), 1:(D_pos/D)}\n","    return p_prior"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":685,"status":"ok","timestamp":1665750262773,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"Ms7OTsAEf30f","outputId":"d70a53e7-3b83-435f-d679-7c3d326b98d5"},"outputs":[{"data":{"text/plain":["{0: 0.6, 1: 0.4}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["\n","compute_prior_prob(train_y)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9cnux4rwgsNK"},"source":["\n","- Tính $V$: Dựa vào `freqs` tính số lượng các từ duy nhất (uniqe words) - gọi là bộ từ điển\n","\n","- Tính $N_{pos}$ và $N_{neg}$: Dựa vào `freqs` dictionary, tính tổng số từ (có thể trùng lặp) xuất hiện trong positive samples $N_{pos}$ và negative samples $N_{neg}$.\n","\n","- Tính tần suất xuất hiện của mỗi từ trong positive samples $freq_{pos}$ và trong negative samples $freq_{neg}$\n","\n","- Tính xác suất likelihood mỗi từ trong bộ từ điển\n","    - Sử dụng hàm `lookup` lấy ra tần suất xuất hiện của từ là positive $freq_{pos}$, và tần xuất xuất hiện của từ là negative $freq_{neg}$\n","- Tính xác suất cho mỗi từ thuộc vào positive sample: $P(W_{pos})$, thuộc vào negative sample $P(W_{neg})$ sử dụng công thức 4 & 5.\n","\n","$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n","$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n","\n","**Note:** Chúng ta lưu trữ likelihood của mỗi từ vào dictionary với key (từ): $W$, value (dictionary): ${0: P(W_{pos}), 1: P(W_{pos})}$"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"_aqBveSjoqCX"},"outputs":[],"source":["\n","def compute_likelihood(freqs):\n","\n","    # Tính V các từ duy nhất xuất hiện trong tập train\n","    vocab = set([key[0] for key in freqs.keys()])\n","    V = len(vocab)\n","\n","    # Tính N_pos: số lượng từ trong positive samples và N_neg: số từ trong negative sample\n","    N_pos = N_neg = 0\n","    # V_pos = V_neg = 0\n","    for keys in freqs.keys():\n","       \n","        if keys[1] > 0:\n","            N_pos += freqs[keys]\n","        else: \n","            N_neg += freqs[keys]\n","\n","            # V_pos += values\n","    print(f'V: {V}, N_pos: {N_pos}, N_neg: {N_neg}')\n","\n","    # Tính likelihood cho mỗi từ trong bộ từ điển\n","    p_likelihood = {}\n","    for word in vocab:\n","        # Lấy tần xuất xuất hiện của mỗi từ là positive hoặc negative\n","        freq_pos = lookup(freqs, word,1)\n","        freq_neg = lookup(freqs, word,0)\n","\n","        # Tính xác suất likelihood của mỗi từ với class positive và negative\n","        p_w_pos = (freq_pos + 1)/ (N_pos +V)\n","        p_w_neg = (freq_neg +1 )/ (N_neg +V)\n","\n","        # Lưu vào p_likelihood dictionary\n","        p_likelihood[word] = {0:p_w_neg, 1:p_w_pos}\n","\n","    \n","    return p_likelihood"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1665750277516,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"0Qc94FJqog-G","outputId":"1c550830-ddcd-4d97-8702-834be5829387"},"outputs":[{"name":"stdout","output_type":"stream","text":["V: 20, N_pos: 9, N_neg: 14\n"]},{"data":{"text/plain":["{'summer': {0: 0.029411764705882353, 1: 0.06896551724137931},\n"," 'the': {0: 0.029411764705882353, 1: 0.10344827586206896},\n"," 'surprises': {0: 0.058823529411764705, 1: 0.034482758620689655},\n"," 'film': {0: 0.029411764705882353, 1: 0.06896551724137931},\n"," 'predictable': {0: 0.058823529411764705, 1: 0.034482758620689655},\n"," 'fun': {0: 0.029411764705882353, 1: 0.06896551724137931},\n"," 'and': {0: 0.08823529411764706, 1: 0.034482758620689655},\n"," 'no': {0: 0.058823529411764705, 1: 0.034482758620689655},\n"," 'energy': {0: 0.058823529411764705, 1: 0.034482758620689655},\n"," 'few': {0: 0.058823529411764705, 1: 0.034482758620689655},\n"," 'most': {0: 0.029411764705882353, 1: 0.06896551724137931},\n"," 'boring': {0: 0.058823529411764705, 1: 0.034482758620689655},\n"," 'lacks': {0: 0.058823529411764705, 1: 0.034482758620689655},\n"," 'of': {0: 0.029411764705882353, 1: 0.06896551724137931},\n"," 'powerful': {0: 0.029411764705882353, 1: 0.06896551724137931},\n"," 'entirely': {0: 0.058823529411764705, 1: 0.034482758620689655},\n"," 'plain': {0: 0.058823529411764705, 1: 0.034482758620689655},\n"," 'very': {0: 0.058823529411764705, 1: 0.06896551724137931},\n"," 'laughs': {0: 0.058823529411764705, 1: 0.034482758620689655},\n"," 'just': {0: 0.058823529411764705, 1: 0.034482758620689655}}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["\n","compute_likelihood(freqs)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Q2patcrx8Gfh"},"outputs":[],"source":["def train_naive_bayes(train_x, train_y):\n","    # Xây dựng từ điển tần suất xuất hiện của từ và nhãn tương ứng\n","    freqs = count_freq_words(train_x, train_y)\n","\n","    # Tính xác suất tiên nghiệm\n","    p_prior = compute_prior_prob(train_y)\n","\n","    # Tính xác suất likelihood\n","    p_likelihood = compute_likelihood(freqs)\n","\n","    return p_prior, p_likelihood"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1665750307564,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"7739wp07pqa6","outputId":"ac0a14c5-4440-41f3-9ad7-5f2d62ced208"},"outputs":[{"name":"stdout","output_type":"stream","text":["V: 20, N_pos: 9, N_neg: 14\n"]},{"data":{"text/plain":["({0: 0.6, 1: 0.4},\n"," {'summer': {0: 0.029411764705882353, 1: 0.06896551724137931},\n","  'the': {0: 0.029411764705882353, 1: 0.10344827586206896},\n","  'surprises': {0: 0.058823529411764705, 1: 0.034482758620689655},\n","  'film': {0: 0.029411764705882353, 1: 0.06896551724137931},\n","  'predictable': {0: 0.058823529411764705, 1: 0.034482758620689655},\n","  'fun': {0: 0.029411764705882353, 1: 0.06896551724137931},\n","  'and': {0: 0.08823529411764706, 1: 0.034482758620689655},\n","  'no': {0: 0.058823529411764705, 1: 0.034482758620689655},\n","  'energy': {0: 0.058823529411764705, 1: 0.034482758620689655},\n","  'few': {0: 0.058823529411764705, 1: 0.034482758620689655},\n","  'most': {0: 0.029411764705882353, 1: 0.06896551724137931},\n","  'boring': {0: 0.058823529411764705, 1: 0.034482758620689655},\n","  'lacks': {0: 0.058823529411764705, 1: 0.034482758620689655},\n","  'of': {0: 0.029411764705882353, 1: 0.06896551724137931},\n","  'powerful': {0: 0.029411764705882353, 1: 0.06896551724137931},\n","  'entirely': {0: 0.058823529411764705, 1: 0.034482758620689655},\n","  'plain': {0: 0.058823529411764705, 1: 0.034482758620689655},\n","  'very': {0: 0.058823529411764705, 1: 0.06896551724137931},\n","  'laughs': {0: 0.058823529411764705, 1: 0.034482758620689655},\n","  'just': {0: 0.058823529411764705, 1: 0.034482758620689655}})"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Kết quả đầu ra thu được khi huấn luận Naive Bayes Classifier\n","p_prior, p_likelihood = train_naive_bayes(train_x, train_y)\n","\n","p_prior, p_likelihood"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EUmfzZwip2x3"},"source":["### 1.4.Dự đoán với các mẫu thử nghiệm\n","- Tính xác suất của mỗi sample (n từ) dựa vào công thức:\n","$$P(0).P(S|0) = P(0).P(w_{1}|0).P(w_{2}|0)...P(w_{n}|0)$$ "]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ob-nYjpIr7Pb"},"outputs":[],"source":["# Ex 4\n","def naive_bayes_predict(sentence, p_prior, p_likelihood):\n","\n","    # Tiền xử lý dữ liệu\n","    words = basic_preprocess(sentence)\n","\n","    # Khởi tạo giá trị xác suất ban đầu là giá trị xác suất tiên nghiệm\n","    p_neg = p_prior[0]\n","    p_pos = p_prior[1]\n","\n","    for word in words:\n","        # Kiểm tra xem word có tồn tại trong p_likelihood hay không\n","        if word in p_likelihood:\n","            p_neg *= p_likelihood[word][0]\n","            p_pos *= p_likelihood[word][1]\n","    return {'prob': {0: p_neg, 1: p_pos}, 'label': 0 if p_neg > p_pos else 1}"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"gd3X74kBAnrK"},"outputs":[{"data":{"text/plain":["{'prob': {0: 6.106248727864848e-05, 1: 3.2801672885317154e-05}, 'label': 0}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["\n","sentence = \"predictable with no fun\"\n","\n","naive_bayes_predict(sentence, p_prior, p_likelihood)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HGFYVt0IBL_0"},"source":["## 2.Naive Bayes Classfier for Sentiment Analysis on Tweets\n","**Phân tích cảm xúc trên tập 1Tweets1 sử dụng thuật toán phân loại Naive Bayes**"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.8.1)\n","Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.2.0)\n","Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.64.1)\n","Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.3)\n","Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.6)\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip available: 22.3.1 -> 23.1.2\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["pip install nltk"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":652,"status":"ok","timestamp":1665750329521,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"KVAHBqV7But_","outputId":"5f82b775-94c0-456b-f0fd-2c50204bbb02"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package twitter_samples to\n","[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package twitter_samples is already up-to-date!\n"]}],"source":["import re\n","import string\n","import nltk\n","import numpy as np\n","nltk.download('twitter_samples')\n","from nltk.corpus import twitter_samples\n","from nltk.tokenize import TweetTokenizer\n","from tqdm import tqdm"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7aC78vV6Bh_x"},"source":["### 2.1.Dowload Dataset"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"C36MnzNxBhdE"},"outputs":[],"source":["# Tải về tập dữ liệu tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n","\n","# Chia thành 2 tập train và test\n","# train: 4000 samples, test: 1000 samples\n","train_pos = all_positive_tweets[:4000]\n","test_pos = all_positive_tweets[4000:]\n","\n","train_neg = all_negative_tweets[:4000]\n","test_neg = all_negative_tweets[4000:]\n","\n","train_x = train_pos + train_neg\n","test_x = test_pos + test_neg\n","\n","# Tạo nhãn negative: 0, positive: 1\n","train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n","test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uRFnS6IaCf_k"},"source":["### 2.2.Tiền xử lý dữ liệu cho tập `Tweets`"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1665750333770,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"v2Dz3mYCrZkZ","outputId":"0e7e0cc5-5f24-4c98-dc00-a3622e0e3d3c"},"outputs":[{"data":{"text/plain":["['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n"," '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n"," '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n"," '@97sides CONGRATS :)',\n"," 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days',\n"," '@BhaktisBanter @PallaviRuhail This one is irresistible :)\\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM',\n"," \"We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\",\n"," '@Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.',\n"," 'Jgh , but we have to go to Bayan :D bye',\n"," 'As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\\n\\nWell… as the name implies :p.']"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["all_positive_tweets[:10]"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"YJ_0y4bEt5RV"},"outputs":[],"source":["# Ex 5\n","def basic_preprocess(text):\n","    # xóa bỏ stock market tickers like $GE\n","    text = re.sub(r'\\$\\w*','',text)\n","    # xóa bỏ old style retweet text \"RT\"\n","    text = re.sub(r'RT[\\s]','', text)\n","    # xóa bỏ hyperlinks\n","    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n","    # xóa bỏ hashtags\n","    text = re.sub(r'#','', text)\n","\n","    # tokenize\n","    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n","    text_tokens = tokenizer.tokenize(text)\n","\n","    text_clean = []\n","    for word in text_tokens:\n","        if word not in string.punctuation:\n","            text_clean.append(word)\n","    #xóa dấu câu, số, ký tự đặc biệt\n","    return text_clean"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1665750400023,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"GQlY1fXNDZaI","outputId":"ad0722c4-7fd7-4d4d-934c-4ef07e35ef25"},"outputs":[{"data":{"text/plain":["['hello', 'there', 'have', 'a', 'great', 'day', 'good', 'morning']"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["\n","example_sentence = \"RT @Twitter @chapagain Hello There! Have a great day. #good #morning http://chapagain.com.np\"\n","basic_preprocess(example_sentence)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DthCUZiZEL3d"},"source":["### 2.3.Huấn luyện Naive Bayes Classifier trên tập `Tweets`"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1353,"status":"ok","timestamp":1665750405814,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"oFYN8KLqEQiZ","outputId":"a704645e-ae54-44d1-d7aa-a5536a3ac5d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["V: 10846, N_pos: 41915, N_neg: 43322\n"]}],"source":["p_prior, p_likelihood = train_naive_bayes(train_x, train_y)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1665750406621,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"wZ1yd1jHEXtm","outputId":"efa87e79-43a9-40da-e9ee-8ef6127b659b"},"outputs":[{"data":{"text/plain":["({0: 0.5, 1: 0.5}, {0: 0.000350760596662236, 1: 0.0028430090407687496})"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["\n","p_prior, p_likelihood['happy']"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4idaGXBhEz6F"},"source":["### 2.4.Dự đoán"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1665750410175,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"4vEDeak7E5Kv","outputId":"dfbebb0b-5457-41c8-cb8d-c04a65298025"},"outputs":[{"data":{"text/plain":["('Bro:U wan cut hair anot,ur hair long Liao bo\\nMe:since ord liao,take it easy lor treat as save $ leave it longer :)\\nBro:LOL Sibei xialan',\n"," 1.0)"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["test_x[0], test_y[0]"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1665750412325,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"EBBit3jiE3iU","outputId":"d2c9e759-681b-4e4b-9f4c-f78293a7642c"},"outputs":[{"data":{"text/plain":["{'prob': {0: 1.0986197570961273e-80, 1: 2.0957369539348183e-79}, 'label': 1}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["naive_bayes_predict(test_x[0], p_prior, p_likelihood)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tt6FeZSsFxOV"},"source":["### 2.5.Đánh giá độ chính xác trên tập test"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"wmihnaRYFwpD"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  0.994\n"]}],"source":["acc = 0\n","for sentence, label in zip(test_x, test_y):\n","\n","    # predic each sentence in test set\n","    pred = naive_bayes_predict(sentence, p_prior, p_likelihood)['label']\n","\n","    # compare predict label with target label\n","    if int(pred) == int(label):\n","        acc += 1\n","\n","print('Accuracy: ', acc/len(test_x))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"nQGEMKVNwlO3"},"source":["## 3.Logistic Regression for Sentiment Analysis on Tweets"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"h3QCFT7XxI6P"},"source":["### 3.1.Download Dataset\n","Tương tự như mục 2.1"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"Yf4Qx4c9xIbz"},"outputs":[],"source":["# Tải về tập dữ liệu tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n","\n","# Chia thành 2 tập train và test\n","# train: 4000 samples, test: 1000 samples\n","train_pos = all_positive_tweets[:4000]\n","test_pos = all_positive_tweets[4000:]\n","\n","train_neg = all_negative_tweets[:4000]\n","test_neg = all_negative_tweets[4000:]\n","\n","train_x = train_pos + train_neg\n","test_x = test_pos + test_neg\n","\n","# Tạo nhãn negative: 0, positive: 1\n","train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n","test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fNj5wW7sxq4O"},"source":["### 3.3.Logistic Regression"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4h6A0zx_xvEk"},"source":["#### Sigmoid\n","The sigmoid function: \n","\n","$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"jvvgQPvryxUk"},"outputs":[],"source":["\n","def sigmoid(z): \n"," \n","\n","    # calculate the sigmoid of z\n","    h = 1/(1+ np.exp(-z))\n","    \n","    return h"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1665750453269,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"TlgxGHCmykTs","outputId":"894af0d9-852c-44a9-d0ed-7a02c49555a6"},"outputs":[{"data":{"text/plain":["(True, True)"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["\n","sigmoid(0) == 0.5, sigmoid(4.92) == 0.9927537604041685"]},{"cell_type":"markdown","metadata":{"id":"6k_Z1YoszL70"},"source":["#### Gradient Descent Function\n","* Số vòng lặp huấn luyện mô hình: `num_iters`\n","* Với mỗi vòng lặp chúng ta sẽ tính `logits-z`, cost và cập nhật trọng số\n","* Số samples training: `m`, số features trên mỗi sample: `n`\n","* Trọng số mô hình:  \n","$$\\mathbf{\\theta} = \\begin{pmatrix}\n","\\theta_0\n","\\\\\n","\\theta_1\n","\\\\ \n","\\theta_2 \n","\\\\ \n","\\vdots\n","\\\\ \n","\\theta_n\n","\\end{pmatrix}$$\n","\n","* Tính `logits-z`:   $$z = \\mathbf{x}\\mathbf{\\theta}$$\n","    * $\\mathbf{x}$ có chiều (m, n+1) \n","    * $\\mathbf{\\theta}$: có chiều (n+1, 1)\n","    * $\\mathbf{z}$: có chiều (m, 1)\n","* Dự đoán y_hat có chiều (m,1):$$\\widehat{y}(z) = sigmoid(z)$$\n","* Cost function $J$:\n","$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n","* Cập nhật `theta`:\n","$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"fBq9id8h3t9c"},"outputs":[],"source":["# Ex 8\n","def gradient_descent(x, y, theta, alpha, num_iters):\n","    \n","\n","    # lấy m số lượng các sample trong matrix x\n","    m = len(x)\n","    \n","    for i in range(0, num_iters):\n","        # Tính z, phép dot product: x và theta\n","        z = np.dot(x, theta)\n","        # Tính y_hat: sigmoid của z\n","        y_hat = sigmoid(z)\n","        # Tính cost function\n","        J = -(y.T @ np.log(y_hat) + (1-y).T @ np.log(1- y_hat))/m\n","        # Cập nhật trọn số theta\n","        theta = theta - (alpha/m)*(x.T @(y_hat -y))\n","\n","   \n","    return J, theta"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1665750463318,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"BhMFCHJG4K_q","outputId":"90e81f0c-cae9-408f-80ac-d2382ca1c6fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Cost 0.6860551249930996\n","Weight [[8.95483666e-08]\n"," [7.01794701e-05]\n"," [4.66109371e-05]]\n"]}],"source":["\n","np.random.seed(1)\n","\n","# X input: 10 x 3, bias là 1\n","tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n","\n","# Y label: 10 x 1\n","tmp_Y = (np.random.rand(10, 1) > 0.5).astype(float)\n","\n","# Apply gradient descent\n","tmp_J, tmp_theta = gradient_descent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 100)\n","print(f\"\\nCost {tmp_J.item()}\")\n","print(f\"Weight {tmp_theta}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fnnZJKKn5Unz"},"source":["### 3.4.Trích xuất các feature\n","Chuyển từ `tweet` sang feature\n","Với mỗi `tweet` sẽ được biểu diễn bởi 2 feature (Dựa vào `freq` tương tự ở mục #1 và #2):\n","- số lượng các positive words\n","- số lượng các negative words"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"oUTAbCZk8b_x"},"outputs":[],"source":["# Ex 9\n","def extract_features(text, freqs):\n","    \n","    # tiền xử lý\n","    word_l = basic_preprocess(text)\n","    \n","    # 3 thành phần: bias, feature 1 và feature 2\n","    x = np.zeros((1, 3)) \n","    \n","    # bias\n","    x[0,0] = 1 \n","    \n","\n","    for word in word_l:\n","        x[0,1] += lookup(freqs,word, 1)\n","        x[0,2] += lookup(freqs,word, 0)\n","        \n","\n","    assert(x.shape == (1, 3))\n","    return x"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1161,"status":"ok","timestamp":1665750476510,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"Ox6yT-z_7bbt","outputId":"ae6dc280-ca28-4d34-a1b5-6086735a681d"},"outputs":[{"name":"stdout","output_type":"stream","text":["#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"]},{"data":{"text/plain":["array([[1.000e+00, 4.722e+03, 1.612e+03]])"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["# Kiểm tra\n","\n","freqs = count_freq_words(train_x, train_y)\n","print(train_x[0])\n","extract_features(train_x[0], freqs)"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":399,"status":"ok","timestamp":1665750479716,"user":{"displayName":"Thái Nguyễn Quốc","userId":"03315292090052971901"},"user_tz":-420},"id":"kVY_tIN58isS","outputId":"98aef96b-6d37-4785-8aa8-275964b9e6e9"},"outputs":[{"data":{"text/plain":["array([[1., 0., 0.]])"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["# Kiểm tra\n","x_test = \"việt nam\"\n","extract_features(x_test, freqs)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6oKSL2rc8xMZ"},"source":["### 3.5.Huấn luyện mô hình Logistic Regression"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"sWoa3OEd8208"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cost 0.23336587194099365.\n","Weight [[ 5.85752278e-08]\n"," [ 5.70181029e-04]\n"," [-5.08635274e-04]]\n"]}],"source":["# Tạo ma trận X có kích thước mxn với n=3 (số features)\n","X = np.zeros((len(train_x), 3))\n","for i in range(len(train_x)):\n","    X[i, :]= extract_features(train_x[i], freqs)\n","\n","Y = np.expand_dims(train_y, 1)\n","\n","# Huấn luyện với số vòng lặp 1500, tốc độ học 1e-6\n","J, theta = gradient_descent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n","print(f\"Cost {J.item()}.\")\n","print(f\"Weight {theta}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UongNMjK9X5v"},"source":["### 3.6.Dự đoán\n","* Tiền xử lý với dữ liệu thử nghiệm\n","* Tính `logits` dựa vào công thức\n","\n","$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"BpS5jvnr-iXY"},"outputs":[],"source":["# Ex 10\n","def predict_tweet(text, freqs, theta):\n","\n","    x = extract_features(text,freqs)\n","\n","    # extract features\n","    # dự đoán\n","    y_pred = sigmoid(x @ theta)\n","    \n","    return y_pred"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"ImETYplt-pc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["happy -> [[0.51894133]]\n","sad -> [[0.4878567]]\n"]}],"source":["tests = [\"happy\", \"sad\"]\n","for t in tests:\n","    pred = predict_tweet(t, freqs, theta)\n","    print(f'{t} -> {pred}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"z4UjFnEt_BUW"},"source":["### 3.7.Đánh giá độ chính xác trên tập test"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"xg2deATX_JXS"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  0.967\n"]}],"source":["acc = 0\n","for sentence, label in zip(test_x, test_y):\n","\n","    # predic each sentence in test set\n","    pred = predict_tweet(sentence, freqs, theta)\n","\n","    if pred > 0.5:\n","        pred_l = 1\n","    else:\n","        pred_l = 0\n","\n","    # compare predict label with target label\n","    if int(pred_l) == int(label):\n","        acc += 1\n","\n","print('Accuracy: ', acc/len(test_x))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM7IzS8FKYNpDZwycxjqa/s","collapsed_sections":[],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"c081257499a92776a7ff5343ee4d85420a7950a6ae97b0cb2aee7a7ddd41b7f2"}}},"nbformat":4,"nbformat_minor":0}
